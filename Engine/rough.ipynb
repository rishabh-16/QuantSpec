{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabhtiwari/QuantSpec_magidec/Engine/utils.py:79: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"mylib::flash_verification\")\n",
      "/home/rishabhtiwari/QuantSpec_magidec/Engine/utils.py:171: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"mylib::flash_decoding\")\n",
      "/home/rishabhtiwari/QuantSpec_magidec/Engine/utils.py:208: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"mylib::custom_func\")\n",
      "/home/rishabhtiwari/QuantSpec_magidec/Engine/utils.py:223: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"mylib::custom_func_2\")\n",
      "/home/rishabhtiwari/QuantSpec_magidec/Engine/utils.py:232: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"mylib::gqa_custom\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/rishabhtiwari\")\n",
    "import torch\n",
    "from QuantSpec_magidec.Engine.backend_quantspec import LMBackend\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(\"/home/rishabhtiwari/cache/checkpoints/meta-llama/Llama-2-7b-hf/model.pth\")\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = 'cuda:2'\n",
    "target_dec_list = [1]\n",
    "draft_dec_list = [1]\n",
    "engine = LMBackend(dtype=DTYPE, device=DEVICE, dec_list=target_dec_list, draft_dec_list=draft_dec_list)\n",
    "engine.load_model(checkpoint_path, use_tp=False, rank_group = [0], group=None, quantize=True, marlin_checkpoint=\"/home/rishabhtiwari/marlin2/marlin/gptq/checkpoint.pt.marlin.g128\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11008])\n",
      "torch.Size([86, 4096])\n",
      "torch.Size([32, 11008])\n"
     ]
    }
   ],
   "source": [
    "print(engine.model.layers[0].feed_forward.w1_quantized.s.shape)\n",
    "print(engine.model.layers[0].feed_forward.w2_quantized.s.shape)\n",
    "print(engine.model.layers[0].feed_forward.w3_quantized.s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import marlin\n",
    "# l = marlin.Layer(4096, 14336, 128)\n",
    "# # l.pack(torch.randn(1, 4096, 14336), torch.randn(1, 14336, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (w1_quantized): Layer()\n",
       "        (w3_quantized): Layer()\n",
       "        (w2_quantized): Layer()\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attention_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_input = torch.randn(1, 1, 11008).to(torch.bfloat16).to('cuda:2')\n",
    "engine.model.layers[0].feed_forward.w2_quantized(random_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1376])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.layers[0].feed_forward.w3_quantized.workspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14336 // 128 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'marlin' has no attribute 'Linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmarlin\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmarlin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'marlin' has no attribute 'Linear'"
     ]
    }
   ],
   "source": [
    "import marlin\n",
    "marlin.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meta-Llama-3.1-8B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.layers[0].feed_forward.w1_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)\n",
       "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w1_quantized): Layer()\n",
       "        (w3_quantized): Layer()\n",
       "        (w2_quantized): Layer()\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attention_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import marlin\n",
    "def replace_linear(module, name_filter=lambda n: True, groupsize=-1, name=''):\n",
    "    \"\"\"Recursively replace all `torch.nn.Linear` layers by empty Marlin layers.\n",
    "    @module: top-level module in which to perform the replacement \n",
    "    @name_filter: lambda indicating if a layer should be replaced\n",
    "    @groupsize: marlin groupsize\n",
    "    @name: root-level name\n",
    "    \"\"\"\n",
    "    if isinstance(module, marlin.Layer):\n",
    "        return\n",
    "    for attr in dir(module):\n",
    "        tmp = getattr(module, attr)\n",
    "        name1 = name + '.' + attr if name != '' else attr\n",
    "        if isinstance(tmp, nn.Linear) and name_filter(name1):\n",
    "            setattr(\n",
    "                module, attr, marlin.Layer(tmp.in_features, tmp.out_features, groupsize=groupsize)\n",
    "            )\n",
    "    for name1, child in module.named_children():\n",
    "        replace_linear(child, name_filter, groupsize=groupsize, name=name + '.' + name1 if name != '' else name1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear(engine.model, name_filter=lambda n: 'feed_forward' in n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (w1): Layer()\n",
       "  (w3): Layer()\n",
       "  (w2): Layer()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_275405/878203640.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  marlin_dict = torch.load(\"/home/rishabhtiwari/cache/llama3_8b_checkpoint.pt.marlin.g128\")\n"
     ]
    }
   ],
   "source": [
    "marlin_dict = torch.load(\"/home/rishabhtiwari/cache/llama3_8b_checkpoint.pt.marlin.g128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.B', 'model.layers.0.self_attn.q_proj.s', 'model.layers.0.self_attn.k_proj.B', 'model.layers.0.self_attn.k_proj.s', 'model.layers.0.self_attn.v_proj.B', 'model.layers.0.self_attn.v_proj.s', 'model.layers.0.self_attn.o_proj.B', 'model.layers.0.self_attn.o_proj.s', 'model.layers.0.mlp.gate_proj.B', 'model.layers.0.mlp.gate_proj.s', 'model.layers.0.mlp.up_proj.B', 'model.layers.0.mlp.up_proj.s', 'model.layers.0.mlp.down_proj.B', 'model.layers.0.mlp.down_proj.s', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.B', 'model.layers.1.self_attn.q_proj.s', 'model.layers.1.self_attn.k_proj.B', 'model.layers.1.self_attn.k_proj.s', 'model.layers.1.self_attn.v_proj.B', 'model.layers.1.self_attn.v_proj.s', 'model.layers.1.self_attn.o_proj.B', 'model.layers.1.self_attn.o_proj.s', 'model.layers.1.mlp.gate_proj.B', 'model.layers.1.mlp.gate_proj.s', 'model.layers.1.mlp.up_proj.B', 'model.layers.1.mlp.up_proj.s', 'model.layers.1.mlp.down_proj.B', 'model.layers.1.mlp.down_proj.s', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.B', 'model.layers.2.self_attn.q_proj.s', 'model.layers.2.self_attn.k_proj.B', 'model.layers.2.self_attn.k_proj.s', 'model.layers.2.self_attn.v_proj.B', 'model.layers.2.self_attn.v_proj.s', 'model.layers.2.self_attn.o_proj.B', 'model.layers.2.self_attn.o_proj.s', 'model.layers.2.mlp.gate_proj.B', 'model.layers.2.mlp.gate_proj.s', 'model.layers.2.mlp.up_proj.B', 'model.layers.2.mlp.up_proj.s', 'model.layers.2.mlp.down_proj.B', 'model.layers.2.mlp.down_proj.s', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.B', 'model.layers.3.self_attn.q_proj.s', 'model.layers.3.self_attn.k_proj.B', 'model.layers.3.self_attn.k_proj.s', 'model.layers.3.self_attn.v_proj.B', 'model.layers.3.self_attn.v_proj.s', 'model.layers.3.self_attn.o_proj.B', 'model.layers.3.self_attn.o_proj.s', 'model.layers.3.mlp.gate_proj.B', 'model.layers.3.mlp.gate_proj.s', 'model.layers.3.mlp.up_proj.B', 'model.layers.3.mlp.up_proj.s', 'model.layers.3.mlp.down_proj.B', 'model.layers.3.mlp.down_proj.s', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.B', 'model.layers.4.self_attn.q_proj.s', 'model.layers.4.self_attn.k_proj.B', 'model.layers.4.self_attn.k_proj.s', 'model.layers.4.self_attn.v_proj.B', 'model.layers.4.self_attn.v_proj.s', 'model.layers.4.self_attn.o_proj.B', 'model.layers.4.self_attn.o_proj.s', 'model.layers.4.mlp.gate_proj.B', 'model.layers.4.mlp.gate_proj.s', 'model.layers.4.mlp.up_proj.B', 'model.layers.4.mlp.up_proj.s', 'model.layers.4.mlp.down_proj.B', 'model.layers.4.mlp.down_proj.s', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.B', 'model.layers.5.self_attn.q_proj.s', 'model.layers.5.self_attn.k_proj.B', 'model.layers.5.self_attn.k_proj.s', 'model.layers.5.self_attn.v_proj.B', 'model.layers.5.self_attn.v_proj.s', 'model.layers.5.self_attn.o_proj.B', 'model.layers.5.self_attn.o_proj.s', 'model.layers.5.mlp.gate_proj.B', 'model.layers.5.mlp.gate_proj.s', 'model.layers.5.mlp.up_proj.B', 'model.layers.5.mlp.up_proj.s', 'model.layers.5.mlp.down_proj.B', 'model.layers.5.mlp.down_proj.s', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.B', 'model.layers.6.self_attn.q_proj.s', 'model.layers.6.self_attn.k_proj.B', 'model.layers.6.self_attn.k_proj.s', 'model.layers.6.self_attn.v_proj.B', 'model.layers.6.self_attn.v_proj.s', 'model.layers.6.self_attn.o_proj.B', 'model.layers.6.self_attn.o_proj.s', 'model.layers.6.mlp.gate_proj.B', 'model.layers.6.mlp.gate_proj.s', 'model.layers.6.mlp.up_proj.B', 'model.layers.6.mlp.up_proj.s', 'model.layers.6.mlp.down_proj.B', 'model.layers.6.mlp.down_proj.s', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.B', 'model.layers.7.self_attn.q_proj.s', 'model.layers.7.self_attn.k_proj.B', 'model.layers.7.self_attn.k_proj.s', 'model.layers.7.self_attn.v_proj.B', 'model.layers.7.self_attn.v_proj.s', 'model.layers.7.self_attn.o_proj.B', 'model.layers.7.self_attn.o_proj.s', 'model.layers.7.mlp.gate_proj.B', 'model.layers.7.mlp.gate_proj.s', 'model.layers.7.mlp.up_proj.B', 'model.layers.7.mlp.up_proj.s', 'model.layers.7.mlp.down_proj.B', 'model.layers.7.mlp.down_proj.s', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.B', 'model.layers.8.self_attn.q_proj.s', 'model.layers.8.self_attn.k_proj.B', 'model.layers.8.self_attn.k_proj.s', 'model.layers.8.self_attn.v_proj.B', 'model.layers.8.self_attn.v_proj.s', 'model.layers.8.self_attn.o_proj.B', 'model.layers.8.self_attn.o_proj.s', 'model.layers.8.mlp.gate_proj.B', 'model.layers.8.mlp.gate_proj.s', 'model.layers.8.mlp.up_proj.B', 'model.layers.8.mlp.up_proj.s', 'model.layers.8.mlp.down_proj.B', 'model.layers.8.mlp.down_proj.s', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.B', 'model.layers.9.self_attn.q_proj.s', 'model.layers.9.self_attn.k_proj.B', 'model.layers.9.self_attn.k_proj.s', 'model.layers.9.self_attn.v_proj.B', 'model.layers.9.self_attn.v_proj.s', 'model.layers.9.self_attn.o_proj.B', 'model.layers.9.self_attn.o_proj.s', 'model.layers.9.mlp.gate_proj.B', 'model.layers.9.mlp.gate_proj.s', 'model.layers.9.mlp.up_proj.B', 'model.layers.9.mlp.up_proj.s', 'model.layers.9.mlp.down_proj.B', 'model.layers.9.mlp.down_proj.s', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.B', 'model.layers.10.self_attn.q_proj.s', 'model.layers.10.self_attn.k_proj.B', 'model.layers.10.self_attn.k_proj.s', 'model.layers.10.self_attn.v_proj.B', 'model.layers.10.self_attn.v_proj.s', 'model.layers.10.self_attn.o_proj.B', 'model.layers.10.self_attn.o_proj.s', 'model.layers.10.mlp.gate_proj.B', 'model.layers.10.mlp.gate_proj.s', 'model.layers.10.mlp.up_proj.B', 'model.layers.10.mlp.up_proj.s', 'model.layers.10.mlp.down_proj.B', 'model.layers.10.mlp.down_proj.s', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.B', 'model.layers.11.self_attn.q_proj.s', 'model.layers.11.self_attn.k_proj.B', 'model.layers.11.self_attn.k_proj.s', 'model.layers.11.self_attn.v_proj.B', 'model.layers.11.self_attn.v_proj.s', 'model.layers.11.self_attn.o_proj.B', 'model.layers.11.self_attn.o_proj.s', 'model.layers.11.mlp.gate_proj.B', 'model.layers.11.mlp.gate_proj.s', 'model.layers.11.mlp.up_proj.B', 'model.layers.11.mlp.up_proj.s', 'model.layers.11.mlp.down_proj.B', 'model.layers.11.mlp.down_proj.s', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.B', 'model.layers.12.self_attn.q_proj.s', 'model.layers.12.self_attn.k_proj.B', 'model.layers.12.self_attn.k_proj.s', 'model.layers.12.self_attn.v_proj.B', 'model.layers.12.self_attn.v_proj.s', 'model.layers.12.self_attn.o_proj.B', 'model.layers.12.self_attn.o_proj.s', 'model.layers.12.mlp.gate_proj.B', 'model.layers.12.mlp.gate_proj.s', 'model.layers.12.mlp.up_proj.B', 'model.layers.12.mlp.up_proj.s', 'model.layers.12.mlp.down_proj.B', 'model.layers.12.mlp.down_proj.s', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.B', 'model.layers.13.self_attn.q_proj.s', 'model.layers.13.self_attn.k_proj.B', 'model.layers.13.self_attn.k_proj.s', 'model.layers.13.self_attn.v_proj.B', 'model.layers.13.self_attn.v_proj.s', 'model.layers.13.self_attn.o_proj.B', 'model.layers.13.self_attn.o_proj.s', 'model.layers.13.mlp.gate_proj.B', 'model.layers.13.mlp.gate_proj.s', 'model.layers.13.mlp.up_proj.B', 'model.layers.13.mlp.up_proj.s', 'model.layers.13.mlp.down_proj.B', 'model.layers.13.mlp.down_proj.s', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.B', 'model.layers.14.self_attn.q_proj.s', 'model.layers.14.self_attn.k_proj.B', 'model.layers.14.self_attn.k_proj.s', 'model.layers.14.self_attn.v_proj.B', 'model.layers.14.self_attn.v_proj.s', 'model.layers.14.self_attn.o_proj.B', 'model.layers.14.self_attn.o_proj.s', 'model.layers.14.mlp.gate_proj.B', 'model.layers.14.mlp.gate_proj.s', 'model.layers.14.mlp.up_proj.B', 'model.layers.14.mlp.up_proj.s', 'model.layers.14.mlp.down_proj.B', 'model.layers.14.mlp.down_proj.s', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.B', 'model.layers.15.self_attn.q_proj.s', 'model.layers.15.self_attn.k_proj.B', 'model.layers.15.self_attn.k_proj.s', 'model.layers.15.self_attn.v_proj.B', 'model.layers.15.self_attn.v_proj.s', 'model.layers.15.self_attn.o_proj.B', 'model.layers.15.self_attn.o_proj.s', 'model.layers.15.mlp.gate_proj.B', 'model.layers.15.mlp.gate_proj.s', 'model.layers.15.mlp.up_proj.B', 'model.layers.15.mlp.up_proj.s', 'model.layers.15.mlp.down_proj.B', 'model.layers.15.mlp.down_proj.s', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.B', 'model.layers.16.self_attn.q_proj.s', 'model.layers.16.self_attn.k_proj.B', 'model.layers.16.self_attn.k_proj.s', 'model.layers.16.self_attn.v_proj.B', 'model.layers.16.self_attn.v_proj.s', 'model.layers.16.self_attn.o_proj.B', 'model.layers.16.self_attn.o_proj.s', 'model.layers.16.mlp.gate_proj.B', 'model.layers.16.mlp.gate_proj.s', 'model.layers.16.mlp.up_proj.B', 'model.layers.16.mlp.up_proj.s', 'model.layers.16.mlp.down_proj.B', 'model.layers.16.mlp.down_proj.s', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.B', 'model.layers.17.self_attn.q_proj.s', 'model.layers.17.self_attn.k_proj.B', 'model.layers.17.self_attn.k_proj.s', 'model.layers.17.self_attn.v_proj.B', 'model.layers.17.self_attn.v_proj.s', 'model.layers.17.self_attn.o_proj.B', 'model.layers.17.self_attn.o_proj.s', 'model.layers.17.mlp.gate_proj.B', 'model.layers.17.mlp.gate_proj.s', 'model.layers.17.mlp.up_proj.B', 'model.layers.17.mlp.up_proj.s', 'model.layers.17.mlp.down_proj.B', 'model.layers.17.mlp.down_proj.s', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.B', 'model.layers.18.self_attn.q_proj.s', 'model.layers.18.self_attn.k_proj.B', 'model.layers.18.self_attn.k_proj.s', 'model.layers.18.self_attn.v_proj.B', 'model.layers.18.self_attn.v_proj.s', 'model.layers.18.self_attn.o_proj.B', 'model.layers.18.self_attn.o_proj.s', 'model.layers.18.mlp.gate_proj.B', 'model.layers.18.mlp.gate_proj.s', 'model.layers.18.mlp.up_proj.B', 'model.layers.18.mlp.up_proj.s', 'model.layers.18.mlp.down_proj.B', 'model.layers.18.mlp.down_proj.s', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.B', 'model.layers.19.self_attn.q_proj.s', 'model.layers.19.self_attn.k_proj.B', 'model.layers.19.self_attn.k_proj.s', 'model.layers.19.self_attn.v_proj.B', 'model.layers.19.self_attn.v_proj.s', 'model.layers.19.self_attn.o_proj.B', 'model.layers.19.self_attn.o_proj.s', 'model.layers.19.mlp.gate_proj.B', 'model.layers.19.mlp.gate_proj.s', 'model.layers.19.mlp.up_proj.B', 'model.layers.19.mlp.up_proj.s', 'model.layers.19.mlp.down_proj.B', 'model.layers.19.mlp.down_proj.s', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.B', 'model.layers.20.self_attn.q_proj.s', 'model.layers.20.self_attn.k_proj.B', 'model.layers.20.self_attn.k_proj.s', 'model.layers.20.self_attn.v_proj.B', 'model.layers.20.self_attn.v_proj.s', 'model.layers.20.self_attn.o_proj.B', 'model.layers.20.self_attn.o_proj.s', 'model.layers.20.mlp.gate_proj.B', 'model.layers.20.mlp.gate_proj.s', 'model.layers.20.mlp.up_proj.B', 'model.layers.20.mlp.up_proj.s', 'model.layers.20.mlp.down_proj.B', 'model.layers.20.mlp.down_proj.s', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.B', 'model.layers.21.self_attn.q_proj.s', 'model.layers.21.self_attn.k_proj.B', 'model.layers.21.self_attn.k_proj.s', 'model.layers.21.self_attn.v_proj.B', 'model.layers.21.self_attn.v_proj.s', 'model.layers.21.self_attn.o_proj.B', 'model.layers.21.self_attn.o_proj.s', 'model.layers.21.mlp.gate_proj.B', 'model.layers.21.mlp.gate_proj.s', 'model.layers.21.mlp.up_proj.B', 'model.layers.21.mlp.up_proj.s', 'model.layers.21.mlp.down_proj.B', 'model.layers.21.mlp.down_proj.s', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.B', 'model.layers.22.self_attn.q_proj.s', 'model.layers.22.self_attn.k_proj.B', 'model.layers.22.self_attn.k_proj.s', 'model.layers.22.self_attn.v_proj.B', 'model.layers.22.self_attn.v_proj.s', 'model.layers.22.self_attn.o_proj.B', 'model.layers.22.self_attn.o_proj.s', 'model.layers.22.mlp.gate_proj.B', 'model.layers.22.mlp.gate_proj.s', 'model.layers.22.mlp.up_proj.B', 'model.layers.22.mlp.up_proj.s', 'model.layers.22.mlp.down_proj.B', 'model.layers.22.mlp.down_proj.s', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.B', 'model.layers.23.self_attn.q_proj.s', 'model.layers.23.self_attn.k_proj.B', 'model.layers.23.self_attn.k_proj.s', 'model.layers.23.self_attn.v_proj.B', 'model.layers.23.self_attn.v_proj.s', 'model.layers.23.self_attn.o_proj.B', 'model.layers.23.self_attn.o_proj.s', 'model.layers.23.mlp.gate_proj.B', 'model.layers.23.mlp.gate_proj.s', 'model.layers.23.mlp.up_proj.B', 'model.layers.23.mlp.up_proj.s', 'model.layers.23.mlp.down_proj.B', 'model.layers.23.mlp.down_proj.s', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.B', 'model.layers.24.self_attn.q_proj.s', 'model.layers.24.self_attn.k_proj.B', 'model.layers.24.self_attn.k_proj.s', 'model.layers.24.self_attn.v_proj.B', 'model.layers.24.self_attn.v_proj.s', 'model.layers.24.self_attn.o_proj.B', 'model.layers.24.self_attn.o_proj.s', 'model.layers.24.mlp.gate_proj.B', 'model.layers.24.mlp.gate_proj.s', 'model.layers.24.mlp.up_proj.B', 'model.layers.24.mlp.up_proj.s', 'model.layers.24.mlp.down_proj.B', 'model.layers.24.mlp.down_proj.s', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.B', 'model.layers.25.self_attn.q_proj.s', 'model.layers.25.self_attn.k_proj.B', 'model.layers.25.self_attn.k_proj.s', 'model.layers.25.self_attn.v_proj.B', 'model.layers.25.self_attn.v_proj.s', 'model.layers.25.self_attn.o_proj.B', 'model.layers.25.self_attn.o_proj.s', 'model.layers.25.mlp.gate_proj.B', 'model.layers.25.mlp.gate_proj.s', 'model.layers.25.mlp.up_proj.B', 'model.layers.25.mlp.up_proj.s', 'model.layers.25.mlp.down_proj.B', 'model.layers.25.mlp.down_proj.s', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.B', 'model.layers.26.self_attn.q_proj.s', 'model.layers.26.self_attn.k_proj.B', 'model.layers.26.self_attn.k_proj.s', 'model.layers.26.self_attn.v_proj.B', 'model.layers.26.self_attn.v_proj.s', 'model.layers.26.self_attn.o_proj.B', 'model.layers.26.self_attn.o_proj.s', 'model.layers.26.mlp.gate_proj.B', 'model.layers.26.mlp.gate_proj.s', 'model.layers.26.mlp.up_proj.B', 'model.layers.26.mlp.up_proj.s', 'model.layers.26.mlp.down_proj.B', 'model.layers.26.mlp.down_proj.s', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.B', 'model.layers.27.self_attn.q_proj.s', 'model.layers.27.self_attn.k_proj.B', 'model.layers.27.self_attn.k_proj.s', 'model.layers.27.self_attn.v_proj.B', 'model.layers.27.self_attn.v_proj.s', 'model.layers.27.self_attn.o_proj.B', 'model.layers.27.self_attn.o_proj.s', 'model.layers.27.mlp.gate_proj.B', 'model.layers.27.mlp.gate_proj.s', 'model.layers.27.mlp.up_proj.B', 'model.layers.27.mlp.up_proj.s', 'model.layers.27.mlp.down_proj.B', 'model.layers.27.mlp.down_proj.s', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.B', 'model.layers.28.self_attn.q_proj.s', 'model.layers.28.self_attn.k_proj.B', 'model.layers.28.self_attn.k_proj.s', 'model.layers.28.self_attn.v_proj.B', 'model.layers.28.self_attn.v_proj.s', 'model.layers.28.self_attn.o_proj.B', 'model.layers.28.self_attn.o_proj.s', 'model.layers.28.mlp.gate_proj.B', 'model.layers.28.mlp.gate_proj.s', 'model.layers.28.mlp.up_proj.B', 'model.layers.28.mlp.up_proj.s', 'model.layers.28.mlp.down_proj.B', 'model.layers.28.mlp.down_proj.s', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.B', 'model.layers.29.self_attn.q_proj.s', 'model.layers.29.self_attn.k_proj.B', 'model.layers.29.self_attn.k_proj.s', 'model.layers.29.self_attn.v_proj.B', 'model.layers.29.self_attn.v_proj.s', 'model.layers.29.self_attn.o_proj.B', 'model.layers.29.self_attn.o_proj.s', 'model.layers.29.mlp.gate_proj.B', 'model.layers.29.mlp.gate_proj.s', 'model.layers.29.mlp.up_proj.B', 'model.layers.29.mlp.up_proj.s', 'model.layers.29.mlp.down_proj.B', 'model.layers.29.mlp.down_proj.s', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.B', 'model.layers.30.self_attn.q_proj.s', 'model.layers.30.self_attn.k_proj.B', 'model.layers.30.self_attn.k_proj.s', 'model.layers.30.self_attn.v_proj.B', 'model.layers.30.self_attn.v_proj.s', 'model.layers.30.self_attn.o_proj.B', 'model.layers.30.self_attn.o_proj.s', 'model.layers.30.mlp.gate_proj.B', 'model.layers.30.mlp.gate_proj.s', 'model.layers.30.mlp.up_proj.B', 'model.layers.30.mlp.up_proj.s', 'model.layers.30.mlp.down_proj.B', 'model.layers.30.mlp.down_proj.s', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.B', 'model.layers.31.self_attn.q_proj.s', 'model.layers.31.self_attn.k_proj.B', 'model.layers.31.self_attn.k_proj.s', 'model.layers.31.self_attn.v_proj.B', 'model.layers.31.self_attn.v_proj.s', 'model.layers.31.self_attn.o_proj.B', 'model.layers.31.self_attn.o_proj.s', 'model.layers.31.mlp.gate_proj.B', 'model.layers.31.mlp.gate_proj.s', 'model.layers.31.mlp.up_proj.B', 'model.layers.31.mlp.up_proj.s', 'model.layers.31.mlp.down_proj.B', 'model.layers.31.mlp.down_proj.s', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marlin_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -427395805, -2041014920, -1522833463,  ...,  1969854105,\n",
       "          -943142709,  1234729076],\n",
       "        [   80444266,  1954973867,  -160643367,  ...,  1500162713,\n",
       "           882283418,  1731696187],\n",
       "        [ 2110164110,  1718056282, -1967593290,  ..., -1882092933,\n",
       "         -1252493735,  1498966756],\n",
       "        ...,\n",
       "        [-1737259831,  2044893053,  -544820389,  ..., -1469539465,\n",
       "          -915895163,  -377911905],\n",
       "        [ 2054572453, -1687630509, -1470728552,  ...,   691583303,\n",
       "          1773821270,  1891409297],\n",
       "        [-1409517093, -1977844776, -1432516168,  ..., -1738298749,\n",
       "         -2037937471,  2016004218]], dtype=torch.int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.layers[0].feed_forward.w1.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map marlin dict keys to engine model keys by replacing feed_forward with mlp\n",
    "for key in marlin_dict.keys():\n",
    "    if 'mlp' in key:\n",
    "        # Get the corresponding engine model key by replacing mlp with feed_forward\n",
    "        engine_key = key.replace('mlp', 'feed_forward')\n",
    "        \n",
    "        # Map the gate_proj, up_proj, down_proj to w1, w2, w3 respectively\n",
    "        if 'gate_proj' in key:\n",
    "            engine_key = engine_key.replace('gate_proj', 'w1_quantized')\n",
    "        elif 'up_proj' in key:\n",
    "            engine_key = engine_key.replace('up_proj', 'w2_quantized') \n",
    "        elif 'down_proj' in key:\n",
    "            engine_key = engine_key.replace('down_proj', 'w3_quantized')\n",
    "            \n",
    "        # Get the layer components from the key\n",
    "        parts = engine_key.split('.')\n",
    "        layer_idx = int(parts[2])\n",
    "        \n",
    "        # Get the corresponding module\n",
    "        layer = engine.model.layers[layer_idx].feed_forward\n",
    "        module = getattr(layer, parts[-2])\n",
    "        \n",
    "        # Load the weights\n",
    "        param_name = parts[-1] # B or s\n",
    "        setattr(module, param_name, marlin_dict[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w3'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/                          llama2_70b_checkpoint.pt.marlin.g128\n",
      "datautils.py                          llama2_7b_checkpoint.pt.marlin.g128\n",
      "eval.py                               llama3_70b_checkpoint.pt.marlin.g128\n",
      "gptq.py                               llama3_8b_checkpoint.pt.marlin.g128\n",
      "llama2.py                             quant.py\n",
      "llama2_13b_checkpoint.pt.marlin.g128  test_llama_load.py\n"
     ]
    }
   ],
   "source": [
    "ls /home/xihc/marlin/gptq/llama3_8b_checkpoint.pt.marlin.g128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings\n",
      "layers\n",
      "layers.0\n",
      "layers.0.attention\n",
      "layers.0.attention.wqkv\n",
      "layers.0.attention.wo\n",
      "layers.0.feed_forward\n",
      "layers.0.feed_forward.w1\n",
      "layers.0.feed_forward.w3\n",
      "layers.0.feed_forward.w2\n",
      "layers.0.ffn_norm\n",
      "layers.0.attention_norm\n",
      "layers.1\n",
      "layers.1.attention\n",
      "layers.1.attention.wqkv\n",
      "layers.1.attention.wo\n",
      "layers.1.feed_forward\n",
      "layers.1.feed_forward.w1\n",
      "layers.1.feed_forward.w3\n",
      "layers.1.feed_forward.w2\n",
      "layers.1.ffn_norm\n",
      "layers.1.attention_norm\n",
      "layers.2\n",
      "layers.2.attention\n",
      "layers.2.attention.wqkv\n",
      "layers.2.attention.wo\n",
      "layers.2.feed_forward\n",
      "layers.2.feed_forward.w1\n",
      "layers.2.feed_forward.w3\n",
      "layers.2.feed_forward.w2\n",
      "layers.2.ffn_norm\n",
      "layers.2.attention_norm\n",
      "layers.3\n",
      "layers.3.attention\n",
      "layers.3.attention.wqkv\n",
      "layers.3.attention.wo\n",
      "layers.3.feed_forward\n",
      "layers.3.feed_forward.w1\n",
      "layers.3.feed_forward.w3\n",
      "layers.3.feed_forward.w2\n",
      "layers.3.ffn_norm\n",
      "layers.3.attention_norm\n",
      "layers.4\n",
      "layers.4.attention\n",
      "layers.4.attention.wqkv\n",
      "layers.4.attention.wo\n",
      "layers.4.feed_forward\n",
      "layers.4.feed_forward.w1\n",
      "layers.4.feed_forward.w3\n",
      "layers.4.feed_forward.w2\n",
      "layers.4.ffn_norm\n",
      "layers.4.attention_norm\n",
      "layers.5\n",
      "layers.5.attention\n",
      "layers.5.attention.wqkv\n",
      "layers.5.attention.wo\n",
      "layers.5.feed_forward\n",
      "layers.5.feed_forward.w1\n",
      "layers.5.feed_forward.w3\n",
      "layers.5.feed_forward.w2\n",
      "layers.5.ffn_norm\n",
      "layers.5.attention_norm\n",
      "layers.6\n",
      "layers.6.attention\n",
      "layers.6.attention.wqkv\n",
      "layers.6.attention.wo\n",
      "layers.6.feed_forward\n",
      "layers.6.feed_forward.w1\n",
      "layers.6.feed_forward.w3\n",
      "layers.6.feed_forward.w2\n",
      "layers.6.ffn_norm\n",
      "layers.6.attention_norm\n",
      "layers.7\n",
      "layers.7.attention\n",
      "layers.7.attention.wqkv\n",
      "layers.7.attention.wo\n",
      "layers.7.feed_forward\n",
      "layers.7.feed_forward.w1\n",
      "layers.7.feed_forward.w3\n",
      "layers.7.feed_forward.w2\n",
      "layers.7.ffn_norm\n",
      "layers.7.attention_norm\n",
      "layers.8\n",
      "layers.8.attention\n",
      "layers.8.attention.wqkv\n",
      "layers.8.attention.wo\n",
      "layers.8.feed_forward\n",
      "layers.8.feed_forward.w1\n",
      "layers.8.feed_forward.w3\n",
      "layers.8.feed_forward.w2\n",
      "layers.8.ffn_norm\n",
      "layers.8.attention_norm\n",
      "layers.9\n",
      "layers.9.attention\n",
      "layers.9.attention.wqkv\n",
      "layers.9.attention.wo\n",
      "layers.9.feed_forward\n",
      "layers.9.feed_forward.w1\n",
      "layers.9.feed_forward.w3\n",
      "layers.9.feed_forward.w2\n",
      "layers.9.ffn_norm\n",
      "layers.9.attention_norm\n",
      "layers.10\n",
      "layers.10.attention\n",
      "layers.10.attention.wqkv\n",
      "layers.10.attention.wo\n",
      "layers.10.feed_forward\n",
      "layers.10.feed_forward.w1\n",
      "layers.10.feed_forward.w3\n",
      "layers.10.feed_forward.w2\n",
      "layers.10.ffn_norm\n",
      "layers.10.attention_norm\n",
      "layers.11\n",
      "layers.11.attention\n",
      "layers.11.attention.wqkv\n",
      "layers.11.attention.wo\n",
      "layers.11.feed_forward\n",
      "layers.11.feed_forward.w1\n",
      "layers.11.feed_forward.w3\n",
      "layers.11.feed_forward.w2\n",
      "layers.11.ffn_norm\n",
      "layers.11.attention_norm\n",
      "layers.12\n",
      "layers.12.attention\n",
      "layers.12.attention.wqkv\n",
      "layers.12.attention.wo\n",
      "layers.12.feed_forward\n",
      "layers.12.feed_forward.w1\n",
      "layers.12.feed_forward.w3\n",
      "layers.12.feed_forward.w2\n",
      "layers.12.ffn_norm\n",
      "layers.12.attention_norm\n",
      "layers.13\n",
      "layers.13.attention\n",
      "layers.13.attention.wqkv\n",
      "layers.13.attention.wo\n",
      "layers.13.feed_forward\n",
      "layers.13.feed_forward.w1\n",
      "layers.13.feed_forward.w3\n",
      "layers.13.feed_forward.w2\n",
      "layers.13.ffn_norm\n",
      "layers.13.attention_norm\n",
      "layers.14\n",
      "layers.14.attention\n",
      "layers.14.attention.wqkv\n",
      "layers.14.attention.wo\n",
      "layers.14.feed_forward\n",
      "layers.14.feed_forward.w1\n",
      "layers.14.feed_forward.w3\n",
      "layers.14.feed_forward.w2\n",
      "layers.14.ffn_norm\n",
      "layers.14.attention_norm\n",
      "layers.15\n",
      "layers.15.attention\n",
      "layers.15.attention.wqkv\n",
      "layers.15.attention.wo\n",
      "layers.15.feed_forward\n",
      "layers.15.feed_forward.w1\n",
      "layers.15.feed_forward.w3\n",
      "layers.15.feed_forward.w2\n",
      "layers.15.ffn_norm\n",
      "layers.15.attention_norm\n",
      "layers.16\n",
      "layers.16.attention\n",
      "layers.16.attention.wqkv\n",
      "layers.16.attention.wo\n",
      "layers.16.feed_forward\n",
      "layers.16.feed_forward.w1\n",
      "layers.16.feed_forward.w3\n",
      "layers.16.feed_forward.w2\n",
      "layers.16.ffn_norm\n",
      "layers.16.attention_norm\n",
      "layers.17\n",
      "layers.17.attention\n",
      "layers.17.attention.wqkv\n",
      "layers.17.attention.wo\n",
      "layers.17.feed_forward\n",
      "layers.17.feed_forward.w1\n",
      "layers.17.feed_forward.w3\n",
      "layers.17.feed_forward.w2\n",
      "layers.17.ffn_norm\n",
      "layers.17.attention_norm\n",
      "layers.18\n",
      "layers.18.attention\n",
      "layers.18.attention.wqkv\n",
      "layers.18.attention.wo\n",
      "layers.18.feed_forward\n",
      "layers.18.feed_forward.w1\n",
      "layers.18.feed_forward.w3\n",
      "layers.18.feed_forward.w2\n",
      "layers.18.ffn_norm\n",
      "layers.18.attention_norm\n",
      "layers.19\n",
      "layers.19.attention\n",
      "layers.19.attention.wqkv\n",
      "layers.19.attention.wo\n",
      "layers.19.feed_forward\n",
      "layers.19.feed_forward.w1\n",
      "layers.19.feed_forward.w3\n",
      "layers.19.feed_forward.w2\n",
      "layers.19.ffn_norm\n",
      "layers.19.attention_norm\n",
      "layers.20\n",
      "layers.20.attention\n",
      "layers.20.attention.wqkv\n",
      "layers.20.attention.wo\n",
      "layers.20.feed_forward\n",
      "layers.20.feed_forward.w1\n",
      "layers.20.feed_forward.w3\n",
      "layers.20.feed_forward.w2\n",
      "layers.20.ffn_norm\n",
      "layers.20.attention_norm\n",
      "layers.21\n",
      "layers.21.attention\n",
      "layers.21.attention.wqkv\n",
      "layers.21.attention.wo\n",
      "layers.21.feed_forward\n",
      "layers.21.feed_forward.w1\n",
      "layers.21.feed_forward.w3\n",
      "layers.21.feed_forward.w2\n",
      "layers.21.ffn_norm\n",
      "layers.21.attention_norm\n",
      "layers.22\n",
      "layers.22.attention\n",
      "layers.22.attention.wqkv\n",
      "layers.22.attention.wo\n",
      "layers.22.feed_forward\n",
      "layers.22.feed_forward.w1\n",
      "layers.22.feed_forward.w3\n",
      "layers.22.feed_forward.w2\n",
      "layers.22.ffn_norm\n",
      "layers.22.attention_norm\n",
      "layers.23\n",
      "layers.23.attention\n",
      "layers.23.attention.wqkv\n",
      "layers.23.attention.wo\n",
      "layers.23.feed_forward\n",
      "layers.23.feed_forward.w1\n",
      "layers.23.feed_forward.w3\n",
      "layers.23.feed_forward.w2\n",
      "layers.23.ffn_norm\n",
      "layers.23.attention_norm\n",
      "layers.24\n",
      "layers.24.attention\n",
      "layers.24.attention.wqkv\n",
      "layers.24.attention.wo\n",
      "layers.24.feed_forward\n",
      "layers.24.feed_forward.w1\n",
      "layers.24.feed_forward.w3\n",
      "layers.24.feed_forward.w2\n",
      "layers.24.ffn_norm\n",
      "layers.24.attention_norm\n",
      "layers.25\n",
      "layers.25.attention\n",
      "layers.25.attention.wqkv\n",
      "layers.25.attention.wo\n",
      "layers.25.feed_forward\n",
      "layers.25.feed_forward.w1\n",
      "layers.25.feed_forward.w3\n",
      "layers.25.feed_forward.w2\n",
      "layers.25.ffn_norm\n",
      "layers.25.attention_norm\n",
      "layers.26\n",
      "layers.26.attention\n",
      "layers.26.attention.wqkv\n",
      "layers.26.attention.wo\n",
      "layers.26.feed_forward\n",
      "layers.26.feed_forward.w1\n",
      "layers.26.feed_forward.w3\n",
      "layers.26.feed_forward.w2\n",
      "layers.26.ffn_norm\n",
      "layers.26.attention_norm\n",
      "layers.27\n",
      "layers.27.attention\n",
      "layers.27.attention.wqkv\n",
      "layers.27.attention.wo\n",
      "layers.27.feed_forward\n",
      "layers.27.feed_forward.w1\n",
      "layers.27.feed_forward.w3\n",
      "layers.27.feed_forward.w2\n",
      "layers.27.ffn_norm\n",
      "layers.27.attention_norm\n",
      "layers.28\n",
      "layers.28.attention\n",
      "layers.28.attention.wqkv\n",
      "layers.28.attention.wo\n",
      "layers.28.feed_forward\n",
      "layers.28.feed_forward.w1\n",
      "layers.28.feed_forward.w3\n",
      "layers.28.feed_forward.w2\n",
      "layers.28.ffn_norm\n",
      "layers.28.attention_norm\n",
      "layers.29\n",
      "layers.29.attention\n",
      "layers.29.attention.wqkv\n",
      "layers.29.attention.wo\n",
      "layers.29.feed_forward\n",
      "layers.29.feed_forward.w1\n",
      "layers.29.feed_forward.w3\n",
      "layers.29.feed_forward.w2\n",
      "layers.29.ffn_norm\n",
      "layers.29.attention_norm\n",
      "layers.30\n",
      "layers.30.attention\n",
      "layers.30.attention.wqkv\n",
      "layers.30.attention.wo\n",
      "layers.30.feed_forward\n",
      "layers.30.feed_forward.w1\n",
      "layers.30.feed_forward.w3\n",
      "layers.30.feed_forward.w2\n",
      "layers.30.ffn_norm\n",
      "layers.30.attention_norm\n",
      "layers.31\n",
      "layers.31.attention\n",
      "layers.31.attention.wqkv\n",
      "layers.31.attention.wo\n",
      "layers.31.feed_forward\n",
      "layers.31.feed_forward.w1\n",
      "layers.31.feed_forward.w3\n",
      "layers.31.feed_forward.w2\n",
      "layers.31.ffn_norm\n",
      "layers.31.attention_norm\n",
      "norm\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "def print_children(module, name=''):\n",
    "    for name1, child in module.named_children():\n",
    "        print(name + '.' + name1 if name != '' else name1)\n",
    "        print_children(child, name + '.' + name1 if name != '' else name1)\n",
    "print_children(engine.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rishabhtiwari/cache/checkpoints/meta-llama/Meta-Llama-3.1-8B')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "query_sequence_length = 1\n",
    "verify_len = 1\n",
    "max_residual_len = 10\n",
    "residual_len = 5\n",
    "\n",
    "attn_mask = torch.arange(max_residual_len).unsqueeze(0).repeat(query_sequence_length, 1)\n",
    "attn_mask -= torch.arange(query_sequence_length).unsqueeze(1).repeat(1, max_residual_len)\n",
    "attn_mask = (attn_mask <= residual_len-query_sequence_length)*1.\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask2 = torch.tril(torch.ones((verify_len, max_residual_len)), diagonal=residual_len-verify_len)\n",
    "attn_mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask2 == attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def draft_update(k_cache, v_cache, k, v, cache_seqlens, qcache_seqlens):\n",
    "    # Create indices for scatter operation\n",
    "    bsz, num_heads, update_len, head_dim = k.shape\n",
    "    seq_len = k_cache.shape[2]\n",
    "    \n",
    "    # Create index tensor for scatter\n",
    "    indices = torch.arange(update_len, device=k.device)\n",
    "    indices = indices.view(1, 1, -1, 1)  # [1, 1, update_len, 1]\n",
    "    indices = indices + qcache_seqlens.view(-1, 1, 1, 1)  # [bsz, 1, update_len, 1]\n",
    "    indices = indices.expand(bsz, num_heads, update_len, head_dim)\n",
    "    print(indices)\n",
    "    \n",
    "    # Perform scatter operations\n",
    "    k_cache.scatter_(dim=2, index=indices, src=k)\n",
    "    v_cache.scatter_(dim=2, index=indices, src=v)\n",
    "    \n",
    "    return k_cache, v_cache\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draft_update(k_cache, v_cache, k, v, cache_seqlens, qcache_seqlens):\n",
    "    bsz = k_cache.shape[0]\n",
    "    assert bsz == 1, \"Batch size > 1 not supported yet\"\n",
    "    \n",
    "    # Calculate start index for the current batch\n",
    "    start_idx = qcache_seqlens[0]  # Using only first batch element since bsz=1\n",
    "    seq_len = k_cache.shape[2]\n",
    "    \n",
    "    # Create a mask for valid positions\n",
    "    # This avoids data-dependent indexing by operating on the full tensor\n",
    "    position_indices = torch.arange(self.k_cache.shape[2], device=k_cache.device)\n",
    "    valid_positions = (position_indices >= start_idx) & (position_indices < start_idx + seq_len)\n",
    "    \n",
    "    # Expand mask to match cache dimensions\n",
    "    mask = valid_positions.view(1, 1, -1, 1).expand_as(self.k_cache[:, :, :, :])\n",
    "    \n",
    "    # Create expanded k_cache and v_cache\n",
    "    expanded_k = k_cache.unsqueeze(2).expand(-1, -1, self.k_cache.shape[2], -1)\n",
    "    expanded_v = v_cache.unsqueeze(2).expand(-1, -1, self.v_cache.shape[2], -1)\n",
    "    \n",
    "    # Use masked operations instead of direct indexing\n",
    "    self.k_cache = torch.where(mask, expanded_k, self.k_cache)\n",
    "    self.v_cache = torch.where(mask, expanded_v, self.v_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_draft_update():\n",
    "    # Create test inputs\n",
    "    batch_size = 2\n",
    "    num_heads = 4\n",
    "    seq_len = 8\n",
    "    head_dim = 16\n",
    "    \n",
    "    # Create sample caches\n",
    "    k_cache = torch.zeros(batch_size, num_heads, seq_len, head_dim)\n",
    "    v_cache = torch.zeros(batch_size, num_heads, seq_len, head_dim)\n",
    "    k = torch.randn(batch_size, num_heads, 1, head_dim)\n",
    "    v = torch.randn(batch_size, num_heads, 1, head_dim)\n",
    "    \n",
    "    # Create sample sequence lengths\n",
    "    cache_seqlens = torch.tensor([6, 4])  # Different lengths for each batch\n",
    "    qcache_seqlens = torch.tensor([2, 1])  # Different starting points\n",
    "    \n",
    "    # Keep copies of original caches\n",
    "    k_cache_orig = k_cache.clone()\n",
    "    v_cache_orig = v_cache.clone()\n",
    "    \n",
    "    # Run the function\n",
    "    k_out, v_out = draft_update(k_cache, v_cache,k,v, cache_seqlens, qcache_seqlens)\n",
    "    \n",
    "    # Verify shapes haven't changed\n",
    "    assert k_out.shape == k_cache_orig.shape\n",
    "    assert v_out.shape == v_cache_orig.shape\n",
    "    \n",
    "    # Verify the updates happened at correct positions\n",
    "    for b in range(batch_size):\n",
    "        start_idx = qcache_seqlens[b]\n",
    "        end_idx = qcache_seqlens[b] + k.shape[2]\n",
    "        \n",
    "        # Check that values were copied correctly\n",
    "        assert torch.allclose(k_out[b, :, start_idx:end_idx], k[b])\n",
    "        assert torch.allclose(v_out[b, :, start_idx:end_idx], v[b])\n",
    "        \n",
    "        # Check that values outside update range remain unchanged\n",
    "        if start_idx > 0:\n",
    "            assert torch.allclose(k_out[b, :, :start_idx], k_cache_orig[b, :, :start_idx])\n",
    "            assert torch.allclose(v_out[b, :, :start_idx], v_cache_orig[b, :, :start_idx])\n",
    "        if end_idx < seq_len:\n",
    "            assert torch.allclose(k_out[b, :, end_idx:], k_cache_orig[b, :, end_idx:])\n",
    "            assert torch.allclose(v_out[b, :, end_idx:], v_cache_orig[b, :, end_idx:])\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_draft_update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847217/3799017049.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True, False, False],\n",
       "        [False, False,  True,  True, False]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 2\n",
    "seq_len = 5\n",
    "\n",
    "mask = torch.arange(seq_len).unsqueeze(0) >= index_list.unsqueeze(1)\n",
    "mask &= torch.arange(seq_len).unsqueeze(0) < (index_list + a.shape[1]).unsqueeze(1)\n",
    "mask = torch.tensor(mask)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5959],\n",
       "        [ 0.0472],\n",
       "        [-0.8919],\n",
       "        [-0.2357]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[mask] = a.view(-1,a.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2664, -1.1244,  0.1311,  1.5670, -0.5227, -0.0628, -0.3672,\n",
       "           0.7776, -0.6443,  0.5735],\n",
       "         [ 1.2952, -1.7487,  0.5188, -0.9836,  0.3290,  0.1861,  1.1023,\n",
       "          -1.0282,  0.1419,  0.3721],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 2.0867, -1.0878, -1.5198, -0.6441,  1.2848,  0.5716, -0.1964,\n",
       "          -1.4325,  0.4747,  0.7528],\n",
       "         [ 0.6980, -0.2796,  0.1953,  0.0589,  1.1897,  0.2222,  0.0208,\n",
       "          -1.0235,  0.4140, -1.9478],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2664, -1.1244,  0.1311,  1.5670, -0.5227, -0.0628, -0.3672,\n",
       "           0.7776, -0.6443,  0.5735],\n",
       "         [ 1.2952, -1.7487,  0.5188, -0.9836,  0.3290,  0.1861,  1.1023,\n",
       "          -1.0282,  0.1419,  0.3721],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 2.0867, -1.0878, -1.5198, -0.6441,  1.2848,  0.5716, -0.1964,\n",
       "          -1.4325,  0.4747,  0.7528],\n",
       "         [ 0.6980, -0.2796,  0.1953,  0.0589,  1.1897,  0.2222,  0.0208,\n",
       "          -1.0235,  0.4140, -1.9478],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.zeros(2, 5, 10)\n",
    "# a = torch.randn(2, 2, 10)\n",
    "\n",
    "index_list = torch.tensor([1, 2]) # 2, 2, 5\n",
    "A[0, 1:1+a.shape[1], :] = a[0] \n",
    "A[1, 2:2+a.shape[1], :] = a[1]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=torch.tensor([1, 2]).view(1, 2, 1).repeat(2,1,5)\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2241, -1.1241, -1.8280, -0.4621,  0.3573],\n",
      "         [-0.3164,  0.9304,  0.2137, -0.8889, -1.4511],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4408,  0.6711,  3.1076, -0.6692,  0.4591],\n",
      "         [-0.6139,  0.2703,  0.3493, -2.3580, -0.8639],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "A.scatter_(dim=1, index=index, src=a)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2241, -1.1241, -1.8280, -0.4621,  0.3573],\n",
       "        [-0.3164,  0.9304,  0.2137, -0.8889, -1.4511],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3)\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "index = torch.tensor([0, 4, 2])\n",
    "# x.index_copy_(0, index, t)\n",
    "# tensor([[ 1.,  2.,  3.],\n",
    "#         [ 0.,  0.,  0.],\n",
    "#         [ 7.,  8.,  9.],\n",
    "#         [ 0.,  0.,  0.],\n",
    "#         [ 4.,  5.,  6.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magicdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
