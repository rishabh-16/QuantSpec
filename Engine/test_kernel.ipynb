{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FP16 Tensor: tensor([ 5.8154e-01, -9.8145e-01, -1.5293e+00,  6.4014e-01,  1.6346e-03,\n",
      "         2.4646e-01, -3.2275e-01, -4.3237e-01, -2.0977e+00, -1.4443e+00],\n",
      "       dtype=torch.float16)\n",
      "8-bit Quantized Tensor: tensor([250., 104.,  53., 255., 196., 218., 165., 155.,   0.,  61.],\n",
      "       dtype=torch.float16)\n",
      "8-bit Dequantized Tensor: tensor([ 0.5864, -0.9810, -1.5283,  0.6401,  0.0067,  0.2429, -0.3262, -0.4333,\n",
      "        -2.0977, -1.4424], dtype=torch.float16)\n",
      "8-bit Quantization Error: tensor(7.9870e-06, dtype=torch.float16)\n",
      "4-bit Quantized Tensor: tensor([15,  6,  3, 15, 12, 13, 10,  9,  0,  3], dtype=torch.int32)\n",
      "4-bit Dequantized Tensor: tensor([0.8824, 0.3529, 0.1765, 0.8824, 0.7059, 0.7647, 0.5882, 0.5294, 0.0000,\n",
      "        0.1765])\n",
      "4-bit Quantization Error: tensor(1.4386)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def gptq_quantize(tensor, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = (1 << num_bits) - 1\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "\n",
    "    # Scale and zero point\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "\n",
    "    # Perform quantization\n",
    "    quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "    dequantized = (quantized - zero_point) * scale\n",
    "\n",
    "    return quantized, dequantized, scale, zero_point\n",
    "\n",
    "def truncate_to_4bit(quantized_8bit):\n",
    "    # Convert to integers for bit manipulation\n",
    "    int_8bit = quantized_8bit.to(torch.int32)\n",
    "\n",
    "    # Extract the 4 most significant bits using bitwise shift\n",
    "    int_4bit = (int_8bit >> 4) & 0xF\n",
    "\n",
    "    # Convert back to float for further calculations\n",
    "    scale_4bit = 15 / 255.0  # Since we use the MSBs, rescale to 4-bit range\n",
    "    dequantized_4bit = int_4bit * scale_4bit\n",
    "    return int_4bit, dequantized_4bit\n",
    "\n",
    "def calculate_error(original, reconstructed):\n",
    "    return torch.mean((original - reconstructed) ** 2)\n",
    "\n",
    "def main():\n",
    "    # Step 1: Generate a random FP16 tensor\n",
    "    tensor = torch.randn(10, dtype=torch.float16)\n",
    "    print(\"Original FP16 Tensor:\", tensor)\n",
    "\n",
    "    # Step 2: 8-bit Quantization\n",
    "    quantized_8bit, dequantized_8bit, scale_8bit, zp_8bit = gptq_quantize(tensor)\n",
    "    print(\"8-bit Quantized Tensor:\", quantized_8bit)\n",
    "    print(\"8-bit Dequantized Tensor:\", dequantized_8bit)\n",
    "\n",
    "    error_8bit = calculate_error(tensor, dequantized_8bit)\n",
    "    print(\"8-bit Quantization Error:\", error_8bit)\n",
    "\n",
    "    # Step 3: Convert to 4-bit using MSBs\n",
    "    quantized_4bit, dequantized_4bit = truncate_to_4bit(quantized_8bit)\n",
    "    print(\"4-bit Quantized Tensor:\", quantized_4bit)\n",
    "    print(\"4-bit Dequantized Tensor:\", dequantized_4bit)\n",
    "\n",
    "    error_4bit = calculate_error(tensor, dequantized_4bit)\n",
    "    print(\"4-bit Quantization Error:\", error_4bit)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FP16 Tensor: tensor([ 0.2395, -1.2275,  0.8218, -0.3242, -0.7002,  0.8267, -0.9517, -1.2881,\n",
      "         1.6172, -0.4983, -0.5249,  0.6733, -1.3516,  1.8643, -0.3481,  0.4285,\n",
      "         0.8452, -0.5103, -0.6406, -1.3027,  0.3516, -1.0391, -0.0948,  3.4883,\n",
      "         0.0701,  0.3892,  0.2957, -0.3340, -0.8672, -0.3958,  0.4685,  0.5503,\n",
      "        -1.2285,  0.2272,  0.6924,  0.2269,  0.7070,  1.2891,  1.6953,  0.1940,\n",
      "        -0.3435, -0.5029,  1.0088,  0.0308, -1.0684, -0.1390,  1.3232, -0.9883,\n",
      "        -1.5957,  1.6719, -0.1266,  0.1552, -0.4001,  0.4436, -1.4746,  1.1240,\n",
      "        -0.3530,  1.0449,  0.1497, -0.8423,  0.5576,  0.2115,  0.8115, -1.2686,\n",
      "        -0.3835, -1.5059,  0.2529, -0.9888, -1.2100, -0.2747, -0.1814, -0.1918,\n",
      "         0.6865,  0.9927,  0.9966,  1.9365, -0.5586, -0.3181, -0.2612,  1.8379,\n",
      "         1.8428, -1.5352,  0.1803,  1.5244,  0.8569, -0.6719, -0.2135,  1.0977,\n",
      "        -0.5669, -0.3411, -0.0305, -2.2754,  0.3923,  0.2622, -0.7246,  0.9468,\n",
      "         0.8062, -0.9258,  1.4521, -0.9082], dtype=torch.float16)\n",
      "8-bit Quantized Tensor: tensor([111.,  46., 137.,  86.,  70., 137.,  59.,  44., 172.,  79.,  77., 130.,\n",
      "         41., 183.,  85., 120., 138.,  78.,  72.,  43., 116.,  55.,  96., 255.,\n",
      "        104., 118., 114.,  86.,  62.,  83., 121., 125.,  46., 111., 131., 111.,\n",
      "        132., 158., 176., 109.,  85.,  78., 145., 102.,  53.,  94., 159.,  57.,\n",
      "         30., 174.,  95., 108.,  83., 120.,  35., 150.,  85., 147., 107.,  63.,\n",
      "        125., 110., 136.,  45.,  84.,  34., 112.,  57.,  47.,  88.,  93.,  92.,\n",
      "        131., 144., 145., 186.,  76.,  87.,  89., 182., 182.,  33., 109., 168.,\n",
      "        138.,  71.,  91., 149.,  76.,  86.,  99.,   0., 118., 112.,  69., 142.,\n",
      "        136.,  60., 165.,  60.], dtype=torch.float16)\n",
      "8-bit Dequantized Tensor: tensor([ 0.2346, -1.2354,  0.8228, -0.3308, -0.6924,  0.8228, -0.9414, -1.2803,\n",
      "         1.6143, -0.4890, -0.5342,  0.6641, -1.3486,  1.8623, -0.3533,  0.4382,\n",
      "         0.8452, -0.5117, -0.6475, -1.3027,  0.3477, -1.0322, -0.1046,  3.4902,\n",
      "         0.0763,  0.3928,  0.3025, -0.3308, -0.8735, -0.3987,  0.4607,  0.5513,\n",
      "        -1.2354,  0.2346,  0.6870,  0.2346,  0.7095,  1.2979,  1.7041,  0.1893,\n",
      "        -0.3533, -0.5117,  1.0039,  0.0311, -1.0771, -0.1498,  1.3203, -0.9863,\n",
      "        -1.5967,  1.6592, -0.1272,  0.1667, -0.3987,  0.4382, -1.4844,  1.1162,\n",
      "        -0.3533,  1.0488,  0.1442, -0.8511,  0.5513,  0.2120,  0.7998, -1.2578,\n",
      "        -0.3760, -1.5068,  0.2573, -0.9863, -1.2129, -0.2854, -0.1725, -0.1951,\n",
      "         0.6870,  0.9810,  1.0039,  1.9307, -0.5566, -0.3081, -0.2629,  1.8398,\n",
      "         1.8398, -1.5293,  0.1893,  1.5234,  0.8452, -0.6699, -0.2177,  1.0938,\n",
      "        -0.5566, -0.3308, -0.0367, -2.2754,  0.3928,  0.2573, -0.7153,  0.9355,\n",
      "         0.7998, -0.9185,  1.4561, -0.9185], dtype=torch.float16)\n",
      "8-bit Quantization Error: tensor(4.5061e-05, dtype=torch.float16)\n",
      "4-bit Quantized Tensor (Truncated): tensor([ 6,  2,  8,  5,  4,  8,  3,  2, 10,  4,  4,  8,  2, 11,  5,  7,  8,  4,\n",
      "         4,  2,  7,  3,  6, 15,  6,  7,  7,  5,  3,  5,  7,  7,  2,  6,  8,  6,\n",
      "         8,  9, 11,  6,  5,  4,  9,  6,  3,  5,  9,  3,  1, 10,  5,  6,  5,  7,\n",
      "         2,  9,  5,  9,  6,  3,  7,  6,  8,  2,  5,  2,  7,  3,  2,  5,  5,  5,\n",
      "         8,  9,  9, 11,  4,  5,  5, 11, 11,  2,  6, 10,  8,  4,  5,  9,  4,  5,\n",
      "         6,  0,  7,  7,  4,  8,  8,  3, 10,  3], dtype=torch.int32)\n",
      "4-bit Dequantized Tensor (Truncated): tensor([0.3529, 0.1176, 0.4706, 0.2941, 0.2353, 0.4706, 0.1765, 0.1176, 0.5882,\n",
      "        0.2353, 0.2353, 0.4706, 0.1176, 0.6471, 0.2941, 0.4118, 0.4706, 0.2353,\n",
      "        0.2353, 0.1176, 0.4118, 0.1765, 0.3529, 0.8824, 0.3529, 0.4118, 0.4118,\n",
      "        0.2941, 0.1765, 0.2941, 0.4118, 0.4118, 0.1176, 0.3529, 0.4706, 0.3529,\n",
      "        0.4706, 0.5294, 0.6471, 0.3529, 0.2941, 0.2353, 0.5294, 0.3529, 0.1765,\n",
      "        0.2941, 0.5294, 0.1765, 0.0588, 0.5882, 0.2941, 0.3529, 0.2941, 0.4118,\n",
      "        0.1176, 0.5294, 0.2941, 0.5294, 0.3529, 0.1765, 0.4118, 0.3529, 0.4706,\n",
      "        0.1176, 0.2941, 0.1176, 0.4118, 0.1765, 0.1176, 0.2941, 0.2941, 0.2941,\n",
      "        0.4706, 0.5294, 0.5294, 0.6471, 0.2353, 0.2941, 0.2941, 0.6471, 0.6471,\n",
      "        0.1176, 0.3529, 0.5882, 0.4706, 0.2353, 0.2941, 0.5294, 0.2353, 0.2941,\n",
      "        0.3529, 0.0000, 0.4118, 0.4118, 0.2353, 0.4706, 0.4706, 0.1765, 0.5882,\n",
      "        0.1765])\n",
      "4-bit Quantization Error (Truncated): tensor(0.7619)\n",
      "4-bit Quantized Tensor (Direct): tensor([ 7.,  3.,  8.,  5.,  4.,  8.,  3.,  3., 10.,  5.,  5.,  8.,  2., 11.,\n",
      "         5.,  7.,  8.,  5.,  4.,  3.,  7.,  3.,  6., 15.,  6.,  7.,  7.,  5.,\n",
      "         4.,  5.,  7.,  7.,  3.,  7.,  8.,  7.,  8.,  9., 10.,  6.,  5.,  5.,\n",
      "         9.,  6.,  3.,  6.,  9.,  3.,  2., 10.,  6.,  6.,  5.,  7.,  2.,  9.,\n",
      "         5.,  9.,  6.,  4.,  7.,  6.,  8.,  3.,  5.,  2.,  7.,  3.,  3.,  5.,\n",
      "         5.,  5.,  8.,  9.,  9., 11.,  4.,  5.,  5., 11., 11.,  2.,  6., 10.,\n",
      "         8.,  4.,  5.,  9.,  4.,  5.,  6.,  0.,  7.,  7.,  4.,  8.,  8.,  4.,\n",
      "        10.,  4.], dtype=torch.float16)\n",
      "4-bit Dequantized Tensor (Direct): tensor([ 0.4143, -1.1230,  0.7983, -0.3542, -0.7388,  0.7983, -1.1230, -1.1230,\n",
      "         1.5674, -0.3542, -0.3542,  0.7983, -1.5068,  1.9512, -0.3542,  0.4143,\n",
      "         0.7983, -0.3542, -0.7388, -1.1230,  0.4143, -1.1230,  0.0300,  3.4883,\n",
      "         0.0300,  0.4143,  0.4143, -0.3542, -0.7388, -0.3542,  0.4143,  0.4143,\n",
      "        -1.1230,  0.4143,  0.7983,  0.4143,  0.7983,  1.1826,  1.5674,  0.0300,\n",
      "        -0.3542, -0.3542,  1.1826,  0.0300, -1.1230,  0.0300,  1.1826, -1.1230,\n",
      "        -1.5068,  1.5674,  0.0300,  0.0300, -0.3542,  0.4143, -1.5068,  1.1826,\n",
      "        -0.3542,  1.1826,  0.0300, -0.7388,  0.4143,  0.0300,  0.7983, -1.1230,\n",
      "        -0.3542, -1.5068,  0.4143, -1.1230, -1.1230, -0.3542, -0.3542, -0.3542,\n",
      "         0.7983,  1.1826,  1.1826,  1.9512, -0.7388, -0.3542, -0.3542,  1.9512,\n",
      "         1.9512, -1.5068,  0.0300,  1.5674,  0.7983, -0.7388, -0.3542,  1.1826,\n",
      "        -0.7388, -0.3542,  0.0300, -2.2754,  0.4143,  0.4143, -0.7388,  0.7983,\n",
      "         0.7983, -0.7388,  1.5674, -0.7388], dtype=torch.float16)\n",
      "4-bit Quantization Error (Direct): tensor(0.0130, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def gptq_quantize(tensor, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = (1 << num_bits) - 1\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "\n",
    "    # Scale and zero point\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "\n",
    "    # Perform quantization\n",
    "    quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "    dequantized = (quantized - zero_point) * scale\n",
    "\n",
    "    return quantized, dequantized, scale, zero_point\n",
    "\n",
    "def truncate_to_4bit(quantized_8bit):\n",
    "    # Convert to integers for bit manipulation\n",
    "    int_8bit = quantized_8bit.to(torch.int32)\n",
    "\n",
    "    # Extract the 4 most significant bits using bitwise shift\n",
    "    int_4bit = (int_8bit >> 4) & 0xF\n",
    "\n",
    "    # Convert back to float for further calculations\n",
    "    scale_4bit = 15 / 255.0  # Since we use the MSBs, rescale to 4-bit range\n",
    "    dequantized_4bit = int_4bit * scale_4bit\n",
    "    return int_4bit, dequantized_4bit\n",
    "\n",
    "def quantize_4bit_direct(tensor):\n",
    "    return gptq_quantize(tensor, num_bits=4)\n",
    "\n",
    "def calculate_error(original, reconstructed):\n",
    "    return torch.mean((original - reconstructed) ** 2)\n",
    "\n",
    "def main():\n",
    "    # Step 1: Generate a random FP16 tensor\n",
    "    tensor = torch.randn(100, dtype=torch.float16)\n",
    "    print(\"Original FP16 Tensor:\", tensor)\n",
    "\n",
    "    # Step 2: 8-bit Quantization\n",
    "    quantized_8bit, dequantized_8bit, scale_8bit, zp_8bit = gptq_quantize(tensor)\n",
    "    print(\"8-bit Quantized Tensor:\", quantized_8bit)\n",
    "    print(\"8-bit Dequantized Tensor:\", dequantized_8bit)\n",
    "\n",
    "    error_8bit = calculate_error(tensor, dequantized_8bit)\n",
    "    print(\"8-bit Quantization Error:\", error_8bit)\n",
    "\n",
    "    # Step 3: Convert to 4-bit using MSBs\n",
    "    quantized_4bit, dequantized_4bit = truncate_to_4bit(quantized_8bit)\n",
    "    print(\"4-bit Quantized Tensor (Truncated):\", quantized_4bit)\n",
    "    print(\"4-bit Dequantized Tensor (Truncated):\", dequantized_4bit)\n",
    "\n",
    "    error_4bit_trunc = calculate_error(tensor, dequantized_4bit)\n",
    "    print(\"4-bit Quantization Error (Truncated):\", error_4bit_trunc)\n",
    "\n",
    "    # Step 4: Direct 4-bit Quantization\n",
    "    quantized_4bit_direct, dequantized_4bit_direct, scale_4bit_direct, zp_4bit_direct = quantize_4bit_direct(tensor)\n",
    "    print(\"4-bit Quantized Tensor (Direct):\", quantized_4bit_direct)\n",
    "    print(\"4-bit Dequantized Tensor (Direct):\", dequantized_4bit_direct)\n",
    "\n",
    "    error_4bit_direct = calculate_error(tensor, dequantized_4bit_direct)\n",
    "    print(\"4-bit Quantization Error (Direct):\", error_4bit_direct)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magicdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
